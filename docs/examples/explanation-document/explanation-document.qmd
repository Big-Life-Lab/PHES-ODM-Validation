---
title: "ODM Validation Tutorial"
format: html
jupyter: python3
---

This document introduces and demonstartes the ODM validation library, part of the validation toolkit.

The document uses [quarto](https://quarto.org/).

We will first install all the library dependencies by running the next chunk in the terminal. Make sure you are in the root of the library directory.

```{bash}
pip install -r requirements.txt
```

The next chunk includes all the setup code for this demo. It requires the [rich](https://github.com/Textualize/rich) library which can be installed by running the command `python -m pip install rich` in the terminal.

```{python}
#| echo: false

import sys
# Import the validation package. In the future you can install it using pip
sys.path.append("../../../src")

# For importing YAML and CSV files
from odm_validation.utils import import_schema, import_dataset

# Some pretty printing library and code
from rich.pretty import pprint
from rich.table import Table
from rich.console import Console
def pprintDictList(dictList, title):
    table = Table(title = title, expand = True)

    dict_keys = dictList[0].keys()
    for column_name in dict_keys:
        table.add_column(column_name)

    for current_dict in dictList:
        row = []
        for column_name in dict_keys:
            row.append(current_dict[column_name])
        table.add_row(*row)

    console = Console()
    console.print(table)
```

The library has two main functions described below:

1. `validate_data`: which is used to validate an ODM dataset using a validation schema
2. `generate_validation_schema`: which is used to generate a validation schema from the ODM dictionary

The next chunk will import these functions.

```{python}
from odm_validation.validation import validate_data, generate_validation_schema
```

Lets start by validating an ODM dataset. We will use just the `sites` table that "contains information about a site; the location where an environmental sample was taken.". The table has a number of columns but we will validate just the `geoLat` and `geoLong` columns. These columns are mandatory in the table.

Validating an ODM dataset requires a validation schema, which contains the rules to validate the data with. For the demo we created a [YAML file](./validation-schemas/sites-schema.yml) which has the validation rules for the sites table. This next chunk will import that file into our environment.

```{python}
validation_schema = import_schema("./validation-schemas/sites-schema.yml")

pprint(validation_schema, expand_all=True)
```

The validation schema has two fields,

* **schemaVersion** which has the version of the ODM the validation schema is for and
* **schema** which has the validation rules

The structure of the validation rules follows a Python library called [cerberus](https://docs.python-cerberus.org/en/stable/schemas.html). Cerberus does all the validation heavy lifting. The PHES-ODM validation package wraps around cerberus to integrate the ODM dataset and the ODM schema into cereberus.

The dataset in the next chunk comes from a [CSV file](./datasets/invalid-sites-dataset.csv) and represents a `sites` table. The dataset is invalid since its missing its `geoLat` column.

```{python}
invalid_odm_dataset = {
    "sites": import_dataset("./datasets/invalid-sites-dataset.csv")
}
pprintDictList(invalid_odm_dataset["sites"], "Invalid ODM Dataset")
```

Consequentially, when we try to validate the dataset with our constructed validation schema it should give us errors.

The `validate_data` function needs two pieces of information, a validation schema and the ODM dataset to validate with it. It returns a validation report for our inputted dataset. The next chunk validates our invalid ODM dataset and prints the report.

```{python}
validation_result = validate_data(
    validation_schema,
    invalid_odm_dataset
)

pprint(validation_result)
```

Take a look at the `errors` field in the report. It contains the list of errors in our dataset, which will be empty for a valid dataset. Here we can see there are two errors, each error saying that we're missing the `geoLat` column.

Each field in the error report object provides metadata to trace where the error came from. The `message` field gives a human readable description of the error. Although most of the fields are self-explanatory, if further clarification is needed, the `errorType` field can be used to dig deeper by finding the specification file for the validation rule it refers to. For example the `missing_mandatory_column` specification can be found in the [repo](../../validation-rules/missing_mandatory_column.md).

The validation report also consists of versioning metadata fields to help us debug any package errors. These are the `data_version`, `schema_version`, and `package_version` fields.

For completeness lets validate a [valid dataset](./datasets/valid-sites-dataset.csv) in the next two chunks.

```{python}
# Import a valid dataset
valid_odm_data = {
    "sites": import_dataset("./datasets/valid-sites-dataset.csv")
}

pprintDictList(valid_odm_data["sites"], "Valid Sites Table")
```

```{python}
# Validate the valid dataset
pprint(
    validate_data(validation_schema, valid_odm_data)
)
```

As we can see, the error list is empty.

Creating your own validation schemas can get tedious due to the breadth of information in the dictionary. With the `generate_validation_schema` function, we can generate a validation schema from the ODM dictionary. The next chunk has a snippet from the [parts sheet](./dictionary/parts-v2.csv) in the dictionary.

```{python}
parts_sheet = import_dataset("./dictionary/parts-v2.csv")

pprintDictList(parts_sheet, 'Version 2 Parts Sheet')
```

The above parts sheet describes,

* A table called `sites`
* and two mandatory columns in the `sites` table called `geoLat` and `geoLong`

We can use the `generate_validation_schema` function to generate a validation schema from the above parts sheet. The function takes two arguments,

* The parts sheet as a list of Python dictionaries
* The version of the ODM dictionary we want to generate the schema for

The next chunk will generate a validation schema from the above parts sheet for version 2 datasets and prints it.

```{python}
validation_schema_2 = generate_validation_schema(parts_sheet, "2.0.0")

pprint(validation_schema_2)
```

As we can see, this generates an identical validation schema as the one we manually created, with one main difference, it includes a `meta` field used to trace back to the row(s) in the parts sheet used to generate the validation rule.

Validating our invalid dataset using the new validation schema returns the same error report as above, except for a new `validationRuleFields`, which is just a copy of the meta field. We can see that by running the next chunk.

```{python}
pprint(validate_data(validation_schema_2, invalid_odm_dataset))
```

We can also generate validation schemas for version 1 of the ODM dictionary. The next two chunks imports a [demo version 1 parts sheet](./dictionary/parts-v1.csv) and creates a validation schema for it.

```{python}
parts_sheet_v1 = import_dataset("./dictionary/parts-v1.csv")

pprintDictList(parts_sheet_v1, "Version 1 Parts Sheet")
```

```{python}
validation_schema_2_v1 = generate_validation_schema(parts_sheet_v1, "1.0.0")

pprint(validation_schema_2_v1)
```

The printed validation schema is once again identical to the previous one except, the table and column names have been replaced with their version 1 equivalents.

Finally, the next two chunks imports an invalid [version 1 ODM datasets](./datasets/invalid-sites-dataset-v1.csv) and validates it using our version 1 validation schema.

```{python}
version_one_invalid_odm_data = {
    "Site": import_dataset("./datasets/invalid-sites-dataset-v1.csv")
}

pprintDictList(version_one_invalid_odm_data["Site"], "Version 1 Sites Table")
```

```{python}
pprint(validate_data(validation_schema_2_v1, version_one_invalid_odm_data))
```

That's the end!

Some final points,

The validation repo will come with validation schemas for every version of the ODM dictionary.

The [validation-rules](../../validation-rules/) folder contains specifications for all the validation rules the library currently supports.

```{python}
import sys
sys.path.append("../src")

# Function for importing files
from odm_validation.utils import import_dataset, import_schema
from odm_validation.doc_utils import import_json

# Some pretty printing functions
from rich.pretty import pprint
from odm_validation.doc_utils import pprint_dict_list

assets_path = "../assets/validation-rules/duplicate-entries-found"
```

# duplicate_entries_found

This rule identifies when there are two identical entries with the same primary key. The rational has been described [here](https://odm.discourse.group/t/duplicate-entries-and-lastedited-field/55). In brief, the ODM can provide a papertrail of updates made to an entry by allowing users to update entries by: 1. Not deleting the old entry 2. Adding a new row for the updated entry and 3. Updating the `lastUpdated` field for the updated entry.

For validation, this rule would be violated if two rows have the same primary key values but non-unique `lastUpdated` values. For example, the following ODM data snippet would fail validation.

```{python}
invalid_dataset = {
    "addresses": import_dataset(assets_path + "/datasets/invalid-dataset-1.csv")
}

pprint_dict_list(invalid_dataset["addresses"], "Invalid Addresses Table")
```

There are two rows (rows 1 and 2) with the same primary key value of `1` but also the same `lastUpdated` value.

The following dataset would also fail validation,

```{python}
invalid_dataset = {
    "addresses": import_dataset(assets_path + "/datasets/invalid-dataset-2.csv")
}

pprint_dict_list(invalid_dataset["addresses"], "Invalid Addresses Table")
```

The following dataset would pass validation,

```{python}
invalid_dataset = {
    "addresses": import_dataset(assets_path + "/datasets/valid-dataset.csv")
}

pprint_dict_list(invalid_dataset["addresses"], "Valid Addresses Table")
```

## Error report

The error report will have the following fields

* **errorType**: duplicate_entries_found
* **tableName**: The name of the table with duplicate entries
* **columnName** The name of the primary key column
* **rowNumbers**: The indexes of the duplicate entries
* **rows** The entries in the table that failed this validation rule
* **validationRuleFields**: The ODM data dictionary rule fields violated by this row
* **message**: Duplicate entries found in rows <row_indexes> with primary key column <column_name> and primary key value <primary_key_value> in table <table_name>

The error report for the first invalid dataset is shown below,

```{python}
error_report_1_file = open(assets_path + "/error-reports/error-report-1.json")
error_report_1_json_str = error_report_1_file.read()
error_report_1_file.close()

pprint(json.loads(error_report_1_json_str), expand_all = True)
```

In addition, a seperate error report object should be generated for each set of duplicate values found. For example the error report for the second invalid dataset above is shown below,

```{python}
error_report_2_file = open(assets_path + "/error-reports/error-report-2.json")
error_report_2_json_str = error_report_2_file.read()
error_report_2_file.close()

pprint(json.loads(error_report_2_json_str), expand_all = True)
```

Two error report objects should be generated, one for rows 2 and 3 and, one for rows 5 and 6.

## Rule metadata

All the metadata for this rule is contained in the parts sheet in the data dictionary. This rule should be added to the primary key column for all tables which can identified using these [instructions](../specs/odm-how-tos.md#how-to-get-the-columns-names-for-a-table).

For example,

```{python}
parts_v2 = import_dataset(assets_path + "/parts/parts-v2.csv")
pprint_dict_list(parts_v2, title = "Parts v2")
```

Here the `addId` part can be identified as being the primary key for the addresses table and hence should have this rule implemented.

## Cerberus Schema

Cerberus currently does not have support for this rule. We will need to [extend the cerberus validator](https://docs.python-cerberus.org/en/stable/customize.html) to accomplish this.

The generated cerberus object for the example above is shown below,

```{python}
schema_v2 = import_schema(assets_path + "/validation-schemas/schema-v2.yml")
pprint(schema_v2)
```

The metadata for this rule should include the row from the ODM that defines the primary key column for the table.

## ODM Version 1

When generating the schema for version 1, we should add this rule to any version 2 primary key columns which have a version 1 equivalent. For example, the parts snippet above with its version 1 columns is shown below

```{python}
parts_v1 = import_dataset(assets_path + "/parts/parts-v1.csv")
pprint_dict_list(parts_v1, title = "Parts v1")
```

The corresponding cerberus schema would be,

```{python}
schema_v1 = import_schema(assets_path + "/validation-schemas/schema-v1.yml")
pprint(schema_v1)
```

The metadata should include the following columns,

* The `partID` column value
* The `<table_name>` column value 
* The `version1Location` column value
* The `version1Table` column value
* The `version1Variable` column value